{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bytedance/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os  # 新增，用于处理路径相关操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用设备{device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, features, targets, movies_df):\n",
    "        \"\"\"\n",
    "        类的初始化\n",
    "        :param features: 特征数据\n",
    "        :param targets: 目标值（评分）\n",
    "        :param movies_df: 电影数据框\n",
    "        \"\"\"\n",
    "        self.features = features # 特征数据\n",
    "        self.targets = targets # 目标值（评分）\n",
    "        # 计算电影热度\n",
    "        self.movie_popularity = movies_df.groupby('movie_id').size() # 统计每部电影的出现次数\n",
    "        self.popular_movies = set(self.movie_popularity.nlargest(1000).index) # 取前1000个热门电影\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        获取数据集的长度\n",
    "        \"\"\"\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        获取单个样本的方法\n",
    "        : param idx: 样本索引\n",
    "        : return: 样本的特征和标签\n",
    "        \"\"\"\n",
    "        uid = torch.tensor(self.features[idx][0]) # 用户ID\n",
    "        movie_id = torch.tensor(self.features[idx][1]) # 电影ID\n",
    "        user_gender = torch.tensor(self.features[idx][2]) # 用户性别\n",
    "        user_age = torch.tensor(self.features[idx][3]) # 用户年龄\n",
    "        user_job = torch.tensor(self.features[idx][4]) # 用户职业\n",
    "        movie_titles = torch.tensor(self.features[idx][6]) # 电影标题\n",
    "        movie_categories = torch.tensor(self.features[idx][7]) # 电影类型\n",
    "        \n",
    "        # 标签处理：评分大于3分的标记为正样本（label=1），否则为负样本（label=0）\n",
    "        label = 1.0 if self.targets[idx] > 3.0 else 0.0\n",
    "        # 将标签转换为张亮\n",
    "        targets = torch.tensor(label).float()\n",
    "        \n",
    "        # 从热门电影集合里随机选择一个作为额外的负样本\n",
    "        negative_movie_id = np.random.choice(list(self.popular_movies))\n",
    "        # 将负样本的电影ID转换为张量\n",
    "        negative_movie = torch.tensor(negative_movie_id)\n",
    "\n",
    "        return uid, movie_id, user_gender, user_age, user_job, movie_titles, movie_categories, targets, negative_movie\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserTower(nn.Module):\n",
    "    def __init__(self, uid_num, gender_num, age_num, job_num, embed_dim):\n",
    "        \"\"\"\n",
    "        初始化函数\n",
    "        :param uid_num: 用户ID的数量\n",
    "        :param gender_num: 性别数量\n",
    "        :param age_num: 年龄数量\n",
    "        :param job_num: 职业数量\n",
    "        :param embed_dim: 嵌入维度\n",
    "        \"\"\"\n",
    "        super(UserTower, self).__init__()\n",
    "        # 创建嵌入层\n",
    "        self.uid_embedding = nn.Embedding(uid_num, embed_dim) # 用户ID嵌入层\n",
    "        self.gender_embedding = nn.Embedding(gender_num, embed_dim // 2) # 性别嵌入层（维度是embed_dim的一半）\n",
    "        self.age_embedding = nn.Embedding(age_num, embed_dim // 2) # 年龄嵌入层（维度是embed_dim的一半）\n",
    "        self.job_embedding = nn.Embedding(job_num, embed_dim // 2) # 职业嵌入层（维度是embed_dim的一半）\n",
    "        # 激活函数\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        # 第一层全连接（为每个特征创建全连接层，将维度统一到embed_dim）\n",
    "        self.uid_fc = nn.Linear(embed_dim, embed_dim)\n",
    "        self.gender_fc = nn.Linear(embed_dim // 2, embed_dim)\n",
    "        self.age_fc = nn.Linear(embed_dim // 2, embed_dim)\n",
    "        self.job_fc = nn.Linear(embed_dim // 2, embed_dim)\n",
    "        # 第二层全连接（将所有的特征组合并通过一个全连接层降维到200维）\n",
    "        self.combine_fc = nn.Linear(4 * embed_dim, 200)\n",
    "\n",
    "    def forward(self, uid, user_gender, user_age, user_job):\n",
    "        \"\"\"\n",
    "        前向传播函数\n",
    "        :param uid: 用户ID\n",
    "        :param user_gender: 用户性别\n",
    "        :param user_age: 用户年龄\n",
    "        :param user_job: 用户职业\n",
    "        :return: 用户特征的嵌入向量\n",
    "        \"\"\"\n",
    "        # 获取嵌入向量\n",
    "        uid_embed = self.uid_embedding(uid) # 用户ID嵌入向量\n",
    "        gender_embed = self.gender_embedding(user_gender) # 性别嵌入向量\n",
    "        age_embed = self.age_embedding(user_age) # 年龄嵌入向量\n",
    "        job_embed = self.job_embedding(user_job) # 职业嵌入向量\n",
    "\n",
    "        # 通过全连接层\n",
    "        uid_fc_output = self.relu(self.uid_fc(uid_embed)) \n",
    "        gender_fc_output = self.relu(self.gender_fc(gender_embed))\n",
    "        age_fc_output = self.relu(self.age_fc(age_embed))\n",
    "        job_fc_output = self.relu(self.job_fc(job_embed))\n",
    "\n",
    "        # 将用户ID、性别、年龄、职业的特征拼接起来，并且拼接的维度是最后一个（都是embed_dim维度，可以参考init）\n",
    "        user_combine = torch.cat([uid_fc_output, gender_fc_output, age_fc_output, job_fc_output], dim=-1)\n",
    "        # 通过Tanh激活函数\n",
    "        user_output = self.tanh(self.combine_fc(user_combine))\n",
    "        # L2正则化,使向量长度为1\n",
    "        user_output = F.normalize(user_output, p=2, dim=1)\n",
    "        return user_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieTower(nn.Module):\n",
    "    def __init__(self, mid_num, movie_category_num, movie_title_num, embed_dim, window_sizes, filter_num, sentence_size, dropout_keep_prob):\n",
    "        \"\"\"\n",
    "        初始化函数\n",
    "        :param mid_num: 电影ID的数量\n",
    "        :param movie_category_num: 电影类型的数量\n",
    "        :param movie_title_num: 电影标题的数量\n",
    "        :param embed_dim: 嵌入维度\n",
    "        :param window_sizes: 窗口大小列表\n",
    "        :param filter_num: 卷积核数量\n",
    "        :param sentence_size: 句子长度\n",
    "        :param dropout_keep_prob: Dropout的保留概率\n",
    "        \"\"\"\n",
    "        super(MovieTower, self).__init__()\n",
    "        # 创建嵌入层\n",
    "        self.movie_id_embedding = nn.Embedding(mid_num, embed_dim) # 电影ID的嵌入层\n",
    "        self.movie_categories_embedding = nn.Embedding(movie_category_num, embed_dim) # 电影类型的嵌入层\n",
    "        self.movie_title_embedding = nn.Embedding(movie_title_num, embed_dim) # 电影标题的嵌入层\n",
    "        # 激活函数\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        # 电影 ID 全连接层\n",
    "        self.movie_id_fc = nn.Linear(embed_dim, embed_dim)\n",
    "        # 电影类型全连接层\n",
    "        self.movie_categories_fc = nn.Linear(embed_dim, embed_dim)\n",
    "        # 卷积层（用于处理电影标题）\n",
    "        self.conv_layers = nn.ModuleList([\n",
    "            nn.Conv2d(1, filter_num, (window_size, embed_dim)) for window_size in window_sizes\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(1 - dropout_keep_prob)\n",
    "        # 第二层全连接\n",
    "        self.combine_fc = nn.Linear(2 * embed_dim + len(window_sizes) * filter_num, 200)\n",
    "\n",
    "    def forward(self, movie_id, movie_categories, movie_titles):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        :param movie_id: 电影ID\n",
    "        :param movie_categories: 电影类型\n",
    "        :param movie_titles: 电影标题\n",
    "        \"\"\" \n",
    "        movie_id_embed = self.movie_id_embedding(movie_id) # 电影ID嵌入向量\n",
    "        movie_categories_embed = self.movie_categories_embedding(movie_categories) # 电影类型嵌入向量\n",
    "        # 对多个类型进行求和（因为一个电影可能对应多个不同的类型）\n",
    "        movie_categories_embed = torch.sum(movie_categories_embed, dim=1)\n",
    "\n",
    "        movie_title_embed = self.movie_title_embedding(movie_titles) # 电影标题嵌入向量\n",
    "        movie_title_embed_expand = movie_title_embed.unsqueeze(1) # 添加通道维度\n",
    "\n",
    "        # 使用CNN处理标题\n",
    "        pool_layer_lst = []\n",
    "        for conv in self.conv_layers:\n",
    "            # 卷积操作\n",
    "            conv_layer = conv(movie_title_embed_expand)\n",
    "            # ReLU激活\n",
    "            relu_layer = self.relu(conv_layer)\n",
    "            # 最大池化\n",
    "            maxpool_layer = nn.functional.max_pool2d(relu_layer, (relu_layer.size(2), relu_layer.size(3)))\n",
    "            pool_layer_lst.append(maxpool_layer)\n",
    "\n",
    "        # 合并所有池化层的输出\n",
    "        pool_layer = torch.cat(pool_layer_lst, dim=1)\n",
    "        pool_layer_flat = pool_layer.view(pool_layer.size(0), -1)\n",
    "\n",
    "        # Dropout层\n",
    "        dropout_layer = self.dropout(pool_layer_flat)\n",
    "\n",
    "        # 处理ID和类别特征\n",
    "        movie_id_fc_output = self.relu(self.movie_id_fc(movie_id_embed)) # 电影ID的Embedding进行全连接层和ReLU激活\n",
    "        movie_categories_fc_output = self.relu(self.movie_categories_fc(movie_categories_embed)) # 电影标题的Embedding经过全连接层和ReLU激活\n",
    "\n",
    "        # 合并电影的ID、类别和标题的Embedding\n",
    "        movie_combine = torch.cat([movie_id_fc_output, movie_categories_fc_output, dropout_layer], dim=-1)\n",
    "        # 将Embedding向量经过全连接层和Tanh激活函数\n",
    "        movie_output = self.tanh(self.combine_fc(movie_combine))\n",
    "        # L2正则化,使向量长度为1\n",
    "        movie_output = F.normalize(movie_output, p=2, dim=1)\n",
    "        return movie_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieRecommendationModel(nn.Module):\n",
    "    def __init__(self, uid_num, gender_num, age_num, job_num, embed_dim, mid_num, movie_category_num, movie_title_num, window_sizes, filter_num, sentence_size, dropout_keep_prob):\n",
    "        \"\"\"\n",
    "        初始化函数\n",
    "        :param uid_num: 用户ID的数量\n",
    "        :param gender_num: 性别数量\n",
    "        :param age_num: 年龄数量\n",
    "        :param job_num: 职业数量\n",
    "        :param embed_dim: 嵌入维度\n",
    "        :param mid_num: 电影ID的数量\n",
    "        :param movie_category_num: 电影类型的数量\n",
    "        :param movie_title_num: 电影标题的数量\n",
    "        :param window_sizes: 文本卷积滑动窗口的大小\n",
    "        :param filter_num: 文本卷积核数量\n",
    "        :param sentence_size: 电影标题的长度\n",
    "        :param dropout_keep_prob: Dropout的保留比例\n",
    "        \"\"\"\n",
    "        super(MovieRecommendationModel, self).__init__()\n",
    "        self.user_tower = UserTower(uid_num, gender_num, age_num, job_num, embed_dim)\n",
    "        self.movie_tower = MovieTower(mid_num, movie_category_num, movie_title_num, embed_dim, window_sizes, filter_num, sentence_size, dropout_keep_prob)\n",
    "        self.temperature = nn.Parameter(torch.tensor(0.07))  # 温度参数,可学习\n",
    "\n",
    "    def forward(self, uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles):\n",
    "        \"\"\"\n",
    "        前向传播函数\n",
    "        :param uid: 用户ID\n",
    "        :param user_gender: 用户性别\n",
    "        :param user_age: 用户年龄\n",
    "        :param user_job: 用户职业\n",
    "        :param movie_id: 电影ID\n",
    "        :param movie_categories: 电影类型\n",
    "        :param movie_titles: 电影标题\n",
    "        :return: 相似度矩阵\n",
    "        \"\"\"\n",
    "        user_output = self.user_tower(uid, user_gender, user_age, user_job)\n",
    "        movie_output = self.movie_tower(movie_id, movie_categories, movie_titles)\n",
    "        # 计算余弦相似度\n",
    "        similarity = torch.matmul(user_output, movie_output.t()) / self.temperature\n",
    "        return similarity, user_output, movie_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_nce_loss(similarity, user_output, movie_output, labels, negative_movie_output, device, margin=0.5):\n",
    "    \"\"\"\n",
    "    计算改进的InfoNCE Loss:\n",
    "    1. 使用in-batch negatives\n",
    "    2. 加入难负样本挖掘\n",
    "    3. 加入热门电影负样本\n",
    "    :param similarity：相似度矩阵\n",
    "    :param user_output：用户表示向量（可以从用户塔获得）\n",
    "    :param movie_output：电影表示向量（可以从电影塔获得）\n",
    "    :param labels：标签（正样本为1，负样本为0）\n",
    "    :param negative_movie_output：热门电影的表示向量\n",
    "    :param device：设备\n",
    "    :param margin：margin参数\n",
    "    :return: 损失值 \n",
    "    \"\"\"\n",
    "    labels = labels.view(-1, 1)  # [B, 1]\n",
    "    pos_mask = torch.eq(labels, labels.T).float().to(device)  # [B, B] 创建正样本掩码\n",
    "    neg_mask = 1 - pos_mask  # [B, B] 创建负样本掩码\n",
    "    \n",
    "    # 计算与热门电影的相似度（使用0.07作为温度参数）\n",
    "    negative_sim = torch.matmul(user_output, negative_movie_output.t()) / 0.07\n",
    "    \n",
    "    # 找出难负样本(相似度高于阈值的负样本）\n",
    "    hard_negative_mask = (similarity > margin) & (neg_mask.bool())\n",
    "    \n",
    "    # 计算正样本的loss\n",
    "    exp_sim = torch.exp(similarity)  # [B, B]\n",
    "    log_prob = similarity - torch.log(exp_sim.sum(dim=1, keepdim=True))  # [B, B]\n",
    "    mean_log_prob_pos = (pos_mask * log_prob).sum(1) / pos_mask.sum(1)  # [B]\n",
    "    \n",
    "    # 计算难负样本的loss\n",
    "    hard_negative_loss = torch.where(hard_negative_mask, similarity, torch.zeros_like(similarity)).mean()\n",
    "    \n",
    "    # 计算热门电影负样本的loss\n",
    "    negative_loss = negative_sim.mean()\n",
    "    \n",
    "    # 总loss：组合三个损失项\n",
    "    # 使用权重0.1平衡难负样本和热门电影负样本的影响\n",
    "    loss = -mean_log_prob_pos.mean() + 0.1 * hard_negative_loss + 0.1 * negative_loss\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, optimizer, num_epochs, show_every_n_batches, writer):\n",
    "    \"\"\"\n",
    "    模型训练代码\n",
    "    :param model：推荐系统模型\n",
    "    :param train_loader：训练数据加载器\n",
    "    :param test_loader：测试数据加载器\n",
    "    :param optimizer：优化器\n",
    "    :param num_epochs：训练轮数\n",
    "    :param show_every_n_batches：每隔多少个batch打印一次训练信息\n",
    "    :param writer：TensorBoard的Writer对象\n",
    "    :return：训练好的模型\n",
    "    \"\"\"\n",
    "    for epoch in range(num_epochs):\n",
    "        # 训练模式\n",
    "        model.train()\n",
    "        for batch_i, (uid, movie_id, user_gender, user_age, user_job, movie_titles, movie_categories, target, negative_movie) in enumerate(\n",
    "                train_loader):\n",
    "            optimizer.zero_grad() # 清空梯度\n",
    "\n",
    "            # 将数据转移到GPU上\n",
    "            uid = uid.to(device)\n",
    "            movie_id = movie_id.to(device)\n",
    "            user_gender = user_gender.to(device)\n",
    "            user_age = user_age.to(device)\n",
    "            user_job = user_job.to(device)\n",
    "            movie_titles = movie_titles.to(device)\n",
    "            movie_categories = movie_categories.to(device)\n",
    "            target = target.to(device)\n",
    "            negative_movie = negative_movie.to(device)\n",
    "\n",
    "            # 前向传播\n",
    "            similarity, user_output, movie_output = model(uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles)\n",
    "            # 获取负样本的电影特征\n",
    "            negative_movie_output = model.movie_tower(negative_movie, movie_categories, movie_titles)\n",
    "            \n",
    "            # 计算损失\n",
    "            loss = info_nce_loss(similarity, user_output, movie_output, target, negative_movie_output, device)\n",
    "\n",
    "            # 反向传播\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 训练信息显示\n",
    "            if (epoch * len(train_loader) + batch_i) % show_every_n_batches == 0:\n",
    "                print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "                      f'Batch [{batch_i}/{len(train_loader)}], '\n",
    "                      f'Loss: {loss.item():.4f}')\n",
    "                global_step = epoch * len(train_loader) + batch_i\n",
    "                writer.add_scalar('Loss/train', loss.item(), global_step)\n",
    "\n",
    "        # 验证模式\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_i, (uid, movie_id, user_gender, user_age, user_job, movie_titles, movie_categories, target, negative_movie) in enumerate(\n",
    "                    test_loader):\n",
    "                # 将数据转移到GPU上\n",
    "                uid = uid.to(device)\n",
    "                movie_id = movie_id.to(device)\n",
    "                user_gender = user_gender.to(device)\n",
    "                user_age = user_age.to(device)\n",
    "                user_job = user_job.to(device)\n",
    "                movie_titles = movie_titles.to(device)\n",
    "                movie_categories = movie_categories.to(device)\n",
    "                target = target.to(device)\n",
    "                negative_movie = negative_movie.to(device)\n",
    "\n",
    "                similarity, user_output, movie_output = model(uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles)\n",
    "                negative_movie_output = model.movie_tower(negative_movie, movie_categories, movie_titles)\n",
    "                \n",
    "                loss = info_nce_loss(similarity, user_output, movie_output, target, negative_movie_output, device)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "            test_loss /= len(test_loader)\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "                  f'Test Loss: {test_loss:.4f}')\n",
    "            writer.add_scalar('Loss/test', test_loss, epoch)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "title2int, title_count, title_set, genres2int, genres_map, features_pd, targets_pd, features, targets_values, ratings_df, users_df, movies_df, data = pickle.load(open('./data/preprocess.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存1000个最热门电影ID\n",
      "\n",
      "前10个最热门电影ID及其出现次数:\n",
      "movie_id\n",
      "2858    3428\n",
      "260     2991\n",
      "1196    2990\n",
      "1210    2883\n",
      "480     2672\n",
      "2028    2653\n",
      "589     2649\n",
      "2571    2590\n",
      "1270    2583\n",
      "593     2578\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 统计每个电影的出现次数\n",
    "movie_counts = data['movie_id'].value_counts()\n",
    "\n",
    "# 获取出现次数最多的前1000个电影ID\n",
    "top_1000_movies = movie_counts.nlargest(1000).index.tolist()\n",
    "\n",
    "# 保存这些电影ID\n",
    "with open('top_1000_movies.pkl', 'wb') as f:\n",
    "    pickle.dump(top_1000_movies, f)\n",
    "\n",
    "print(f\"已保存{len(top_1000_movies)}个最热门电影ID\")\n",
    "print(\"\\n前10个最热门电影ID及其出现次数:\")\n",
    "print(movie_counts.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20250416\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "# 获取今天的日期\n",
    "today = datetime.today()\n",
    "\n",
    "# 以 YYYYMMDD 格式输出日期\n",
    "formatted_date = today.strftime(\"%Y%m%d\")\n",
    "print(formatted_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uid num:6041\n",
      "gender num:2\n",
      "age num:7\n",
      "job num:21\n",
      "movie id num:3953\n",
      "movie category num:19\n",
      "movie title num:5217\n",
      "开始划分数据集！\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32\n",
    "# 用户 ID 个数\n",
    "uid_num = max(features.take(0, 1)) + 1\n",
    "# 性别个数\n",
    "gender_num = max(features.take(2, 1)) + 1\n",
    "# 年龄类别个数\n",
    "age_num = max(features.take(3, 1)) + 1\n",
    "# 职业个数\n",
    "job_num = max(features.take(4, 1)) + 1\n",
    "\n",
    "# 电影 ID 个数\n",
    "mid_num = max(features.take(1, 1)) + 1\n",
    "# 电影类型个数\n",
    "movie_category_num = max(genres2int.values()) + 1\n",
    "# 电影名单词个数\n",
    "movie_title_num = len(title_set)\n",
    "\n",
    "print(f\"uid num:{uid_num}\")\n",
    "print(f\"gender num:{gender_num}\")\n",
    "print(f\"age num:{age_num}\")\n",
    "print(f\"job num:{job_num}\")\n",
    "print(f\"movie id num:{mid_num}\")\n",
    "print(f\"movie category num:{movie_category_num}\")\n",
    "print(f\"movie title num:{movie_title_num}\")\n",
    "# 对电影类型的 embedding 向量做 sum 操作\n",
    "combiner = \"sum\"\n",
    "\n",
    "# 电影名长度\n",
    "sentence_size = title_count\n",
    "# 文本卷积滑动窗口\n",
    "window_sizes = {2, 3, 4, 5}\n",
    "# 文本卷积核数量\n",
    "filter_num = 8\n",
    "\n",
    "# 定义超参数\n",
    "num_epochs = 1\n",
    "batch_size = 256\n",
    "dropout_keep_prob = 0.5\n",
    "learning_rate = 0.0001\n",
    "show_every_n_batches = 20\n",
    "# 处理保存路径\n",
    "save_dir = os.path.join(os.getcwd(), \"model_save\")  # 使用当前工作目录下的 model_save 文件夹\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "log_dir = './runs/movie_recommendation_logs'\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "# 划分数据集\n",
    "print(\"开始划分数据集！\")\n",
    "train_features, test_features, train_targets, test_targets = train_test_split(features, targets_values, test_size=0.2,\n",
    "                                                                                random_state=42)\n",
    "\n",
    "# 保存数据到本地\n",
    "pickle.dump((train_features, train_targets, test_features, test_targets), open('./data/split_dataset.p', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始构建训练集和测试集！\n"
     ]
    }
   ],
   "source": [
    "# 构建训练集和测试集\n",
    "print(\"开始构建训练集和测试集！\")\n",
    "train_dataset = MovieDataset(train_features, train_targets, movies_df)\n",
    "test_dataset = MovieDataset(test_features, test_targets, movies_df)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始构建模型！\n"
     ]
    }
   ],
   "source": [
    "print(\"开始构建模型！\")\n",
    "model = MovieRecommendationModel(uid_num, gender_num, age_num, job_num, embed_dim, mid_num, movie_category_num,\n",
    "                                    movie_title_num, window_sizes, filter_num, sentence_size, dropout_keep_prob)\n",
    "# 将模型转移到GPU上\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练模型！\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'MovieDataset' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m开始训练模型！\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_every_n_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, test_loader, optimizer, num_epochs, show_every_n_batches, writer)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# 训练模式\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_i, (uid, movie_id, user_gender, user_age, user_job, movie_titles, movie_categories, target, negative_movie) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     18\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \u001b[38;5;66;03m# 清空梯度\u001b[39;00m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;66;03m# 将数据转移到GPU上\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:491\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:422\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/data/dataloader.py:1146\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1139\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1146\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/context.py:284\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinel \u001b[38;5;241m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetbuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"开始训练模型！\")\n",
    "trained_model = train_model(model, train_loader, test_loader, optimizer, num_epochs, show_every_n_batches, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取今天的日期\n",
    "today=datetime.today()\n",
    "formatted_date=today.strftime('%Y%m%d')\n",
    "model_path = os.path.join(save_dir, f\"two_tower_model_{formatted_date}.pth\")  # 具体的模型保存文件路径（根据日期进行区分）\n",
    "torch.save(trained_model.state_dict(), model_path)\n",
    "print('Model Trained and Saved')\n",
    "\n",
    "# 关闭 tensorboard writer\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
